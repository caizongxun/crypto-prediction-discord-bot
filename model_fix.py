#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹æ¶æ§‹è¨ºæ–·å’Œä¿®å¾©å·¥å…·

å•é¡Œ: ä¿å­˜çš„æ¨¡å‹ (checkpoint) ä½¿ç”¨ä¸åŒçš„éš±è—å±¤å¤§å°
  - å¤§å¤šæ•¸æ¨¡å‹: LSTM hidden_size=256 (é›™å‘ = 512)
  - ç•¶å‰æ¶æ§‹: LSTM hidden_size=128 (é›™å‘ = 256)

è§£æ±ºæ–¹æ¡ˆ:
  1. å‹•æ…‹æª¢æ¸¬æ¨¡å‹å°ºå¯¸ä¸¦é‡æ–°åˆå§‹åŒ–
  2. ä½¿ç”¨å·å­—å…¸æ˜ å°„è¼‰å…¥
  3. æˆ–é‡æ–°è¨“ç·´æ¨¡å‹
"""

import torch
import torch.nn as nn
from pathlib import Path
import json
import logging
from typing import Dict, Tuple, Optional

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CryptoLSTMModel(nn.Module):
    """å‹•æ…‹ LSTM æ¨¡å‹æ¶æ§‹"""
    
    def __init__(self, input_size: int = 44, hidden_size: int = 128, 
                 num_layers: int = 2, output_size: int = 1, 
                 dropout: float = 0.3, bidirectional: bool = True):
        super(CryptoLSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        
        # LSTM å±¤
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional
        )
        
        # å›æ­¸å±¤
        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size
        self.regressor = nn.Sequential(
            nn.Linear(lstm_output_size, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, output_size)
        )
    
    def forward(self, x):
        lstm_out, (h_n, c_n) = self.lstm(x)
        # å–æœ€å¾Œä¸€æ­¥è¼¸å‡º
        last_output = lstm_out[:, -1, :]
        output = self.regressor(last_output)
        return output


class ModelDiagnostic:
    """æ¨¡å‹è¨ºæ–·å·¥å…·"""
    
    @staticmethod
    def analyze_checkpoint(checkpoint_path: str) -> Dict:
        """
        åˆ†ææª¢æŸ¥é»ä¸­çš„æ¨¡å‹å¤§å°
        
        Args:
            checkpoint_path: .pth æ–‡ä»¶è·¯å¾‘
            
        Returns:
            åŒ…å«æ¨¡å‹ä¿¡æ¯çš„å­—å…¸
        """
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            state_dict = checkpoint if isinstance(checkpoint, dict) and 'state_dict' not in checkpoint else checkpoint.get('state_dict', checkpoint)
            
            info = {
                'path': checkpoint_path,
                'lstm_weights': {},
                'regressor_shapes': {}
            }
            
            # åˆ†æ LSTM å±¤
            for key, param in state_dict.items():
                if 'lstm' in key:
                    info['lstm_weights'][key] = tuple(param.shape)
                elif 'regressor' in key:
                    info['regressor_shapes'][key] = tuple(param.shape)
            
            return info
        except Exception as e:
            logger.error(f"âŒ ç„¡æ³•åˆ†ææª¢æŸ¥é»: {e}")
            return {}
    
    @staticmethod
    def get_hidden_size_from_checkpoint(checkpoint_path: str) -> Optional[int]:
        """
        å¾æª¢æŸ¥é»æ¨æ–·éš±è—å±¤å¤§å°
        
        Args:
            checkpoint_path: .pth æ–‡ä»¶è·¯å¾‘
            
        Returns:
            éš±è—å±¤å¤§å°ï¼Œå¦‚æœç„¡æ³•ç¢ºå®šå‰‡è¿”å› None
        """
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            state_dict = checkpoint if isinstance(checkpoint, dict) and 'state_dict' not in checkpoint else checkpoint.get('state_dict', checkpoint)
            
            # æŸ¥æ‰¾ LSTM æ¬Šé‡ä¾†æ¨æ–·éš±è—å±¤å¤§å°
            for key, param in state_dict.items():
                if 'lstm.weight_ih_l0' in key:
                    # LSTM è¼¸å…¥åˆ°éš±è—å±¤çš„æ¬Šé‡å¤§å°æ˜¯ (4*hidden_size, input_size)
                    hidden_size = param.shape[0] // 4
                    logger.info(f"ğŸ“Š æ¨æ–·éš±è—å±¤å¤§å°: {hidden_size} (å¾ {key} æ¬Šé‡ {param.shape})")
                    return hidden_size
            
            return None
        except Exception as e:
            logger.error(f"âŒ æ¨æ–·éš±è—å±¤å¤§å°å¤±æ•—: {e}")
            return None


class ModelLoader:
    """æ™ºèƒ½æ¨¡å‹è¼‰å…¥å™¨"""
    
    @staticmethod
    def load_model_flexible(checkpoint_path: str, target_hidden_size: int = 128,
                           map_location: str = 'cpu') -> Optional[torch.nn.Module]:
        """
        éˆæ´»è¼‰å…¥æ¨¡å‹ï¼Œè‡ªå‹•é©é…ä¸åŒçš„éš±è—å±¤å¤§å°
        
        Args:
            checkpoint_path: æ¨¡å‹æª¢æŸ¥é»è·¯å¾‘
            target_hidden_size: ç›®æ¨™éš±è—å±¤å¤§å°
            map_location: PyTorch è¨­å‚™ä½ç½®
            
        Returns:
            è¼‰å…¥çš„æ¨¡å‹æˆ– None
        """
        try:
            # åˆ†ææª¢æŸ¥é»
            diagnostic = ModelDiagnostic()
            checkpoint_hidden_size = diagnostic.get_hidden_size_from_checkpoint(checkpoint_path)
            
            if checkpoint_hidden_size is None:
                logger.warning(f"âš ï¸  ç„¡æ³•æ¨æ–·éš±è—å±¤å¤§å°ï¼Œä½¿ç”¨ç›®æ¨™å¤§å°: {target_hidden_size}")
                checkpoint_hidden_size = target_hidden_size
            
            logger.info(f"\nğŸ“‹ è¼‰å…¥æ¨¡å‹ä¿¡æ¯:")
            logger.info(f"   æª¢æŸ¥é»éš±è—å±¤: {checkpoint_hidden_size}")
            logger.info(f"   ç›®æ¨™éš±è—å±¤: {target_hidden_size}")
            
            # è¼‰å…¥æª¢æŸ¥é»
            checkpoint = torch.load(checkpoint_path, map_location=map_location)
            state_dict = checkpoint if isinstance(checkpoint, dict) and 'state_dict' not in checkpoint else checkpoint.get('state_dict', checkpoint)
            
            # å¦‚æœå¤§å°åŒ¹é…ï¼Œç›´æ¥è¼‰å…¥
            if checkpoint_hidden_size == target_hidden_size:
                logger.info(f"âœ… éš±è—å±¤å¤§å°åŒ¹é…ï¼Œç›´æ¥è¼‰å…¥")
                model = CryptoLSTMModel(hidden_size=target_hidden_size)
                model.load_state_dict(state_dict, strict=False)
                return model
            
            # å¦‚æœå¤§å°ä¸åŒ¹é…ï¼Œéœ€è¦å¤§å‹æ¨¡å‹
            logger.info(f"ğŸ”„ éš±è—å±¤å¤§å°ä¸åŒ¹é…ï¼Œä½¿ç”¨ {checkpoint_hidden_size} è¼‰å…¥")
            model = CryptoLSTMModel(hidden_size=checkpoint_hidden_size)
            model.load_state_dict(state_dict, strict=False)
            
            # å¯é¸: é‡åŒ–åˆ°è¼ƒå°çš„æ¨¡å‹
            if checkpoint_hidden_size > target_hidden_size:
                logger.info(f"ğŸ“‰ æ­£åœ¨å°‡æ¨¡å‹å¾ {checkpoint_hidden_size} é‡åŒ–åˆ° {target_hidden_size}...")
                model = ModelLoader._quantize_model(model, target_hidden_size)
            
            return model
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    @staticmethod
    def _quantize_model(model: torch.nn.Module, target_hidden_size: int) -> torch.nn.Module:
        """
        å°‡æ¨¡å‹å¾å¤§éš±è—å±¤é‡åŒ–åˆ°å°éš±è—å±¤
        
        Args:
            model: åŸå§‹æ¨¡å‹
            target_hidden_size: ç›®æ¨™éš±è—å±¤å¤§å°
            
        Returns:
            é‡åŒ–å¾Œçš„æ¨¡å‹
        """
        # æ–°å»ºè¼ƒå°çš„æ¨¡å‹
        small_model = CryptoLSTMModel(hidden_size=target_hidden_size)
        
        # è¤‡è£½å¯ä»¥ç›´æ¥æ˜ å°„çš„å±¤
        try:
            # è¤‡è£½ LSTM çš„éƒ¨åˆ†æ¬Šé‡ (ç°¡å–®çš„æ–¹æ³•æ˜¯å¹³å‡)
            for (name_src, param_src), (name_tgt, param_tgt) in zip(
                model.named_parameters(), small_model.named_parameters()
            ):
                if name_src == name_tgt:
                    if param_src.shape == param_tgt.shape:
                        param_tgt.data.copy_(param_src.data)
                    else:
                        # ç°¡å–®çš„å°ºå¯¸èª¿æ•´ (å¯ä»¥æ”¹é€²)
                        if param_src.dim() >= 2:
                            param_tgt.data.copy_(param_src.data[:param_tgt.shape[0], :param_tgt.shape[1]] 
                                               if param_src.shape[0] >= param_tgt.shape[0] else param_src.data)
            
            logger.info(f"âœ… é‡åŒ–å®Œæˆ: {model.hidden_size} -> {target_hidden_size}")
        except Exception as e:
            logger.warning(f"âš ï¸  é‡åŒ–éç¨‹ä¸­å‡ºéŒ¯: {e}")
        
        return small_model


def main():
    """
    ä¸»è¨ºæ–·å‡½æ•¸
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="ğŸ” æ¨¡å‹æ¶æ§‹è¨ºæ–·å·¥å…·")
    parser.add_argument('-a', '--analyze', type=str, help='åˆ†ææª¢æŸ¥é»æ–‡ä»¶')
    parser.add_argument('-l', '--load', type=str, help='è¼‰å…¥ä¸¦æ¸¬è©¦æ¨¡å‹')
    parser.add_argument('-d', '--directory', type=str, default='./models', help='æ¨¡å‹ç›®éŒ„')
    parser.add_argument('-hs', '--hidden-size', type=int, default=128, help='ç›®æ¨™éš±è—å±¤å¤§å°')
    
    args = parser.parse_args()
    
    diagnostic = ModelDiagnostic()
    loader = ModelLoader()
    
    print("\n" + "="*80)
    print("ğŸ” æ¨¡å‹æ¶æ§‹è¨ºæ–·å·¥å…·")
    print("="*80)
    
    # åˆ†æå–®å€‹æª¢æŸ¥é»
    if args.analyze:
        print(f"\nğŸ“Š åˆ†æ: {args.analyze}")
        print("-" * 80)
        info = diagnostic.analyze_checkpoint(args.analyze)
        print(f"\nğŸ“‹ LSTM æ¬Šé‡:")
        for key, shape in info.get('lstm_weights', {}).items():
            print(f"  {key}: {shape}")
        print(f"\nğŸ“‹ å›æ­¸å±¤:")
        for key, shape in info.get('regressor_shapes', {}).items():
            print(f"  {key}: {shape}")
        
        # æ¨æ–·éš±è—å±¤å¤§å°
        hidden_size = diagnostic.get_hidden_size_from_checkpoint(args.analyze)
        print(f"\nâœ… æ¨æ–·çš„éš±è—å±¤å¤§å°: {hidden_size}")
    
    # è¼‰å…¥ä¸¦æ¸¬è©¦æ¨¡å‹
    elif args.load:
        print(f"\nğŸ”„ è¼‰å…¥: {args.load}")
        print("-" * 80)
        model = loader.load_model_flexible(args.load, target_hidden_size=args.hidden_size)
        if model:
            print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            print(f"\nğŸ“Š æ¨¡å‹æ¶æ§‹:")
            print(model)
    
    # æƒæç›®éŒ„ä¸­çš„æ‰€æœ‰æ¨¡å‹
    else:
        print(f"\nğŸ“ æƒæç›®éŒ„: {args.directory}")
        print("-" * 80)
        
        model_dir = Path(args.directory)
        model_files = list(model_dir.glob('*_model_*.pth')) + list(model_dir.glob('*.pth'))
        
        print(f"\næ‰¾åˆ° {len(model_files)} å€‹æ¨¡å‹æ–‡ä»¶\n")
        
        results = []
        for model_file in sorted(model_files):
            hidden_size = diagnostic.get_hidden_size_from_checkpoint(str(model_file))
            results.append({
                'name': model_file.name,
                'hidden_size': hidden_size
            })
        
        print(f"{'æ¨¡å‹':<30} {'éš±è—å±¤':<12} {'ç‹€æ…‹'}")
        print("-" * 60)
        
        for result in results:
            status = "âœ…" if result['hidden_size'] == args.hidden_size else "âŒ"
            print(f"{result['name']:<30} {result['hidden_size']:<12} {status}")
        
        # çµ±è¨ˆ
        print("\n" + "-" * 60)
        print(f"\nğŸ“Š çµ±è¨ˆ:")
        size_groups = {}
        for result in results:
            size = result['hidden_size']
            size_groups[size] = size_groups.get(size, 0) + 1
        
        for size, count in sorted(size_groups.items()):
            match = "âœ… åŒ¹é…" if size == args.hidden_size else "âŒ ä¸åŒ¹é…"
            print(f"  éš±è—å±¤ {size}: {count} å€‹æ¨¡å‹ {match}")
    
    print("\n" + "="*80 + "\n")


if __name__ == '__main__':
    main()
